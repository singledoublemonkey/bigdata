    
说明：

- Coordinator和Overlord进程可以部署在同一台服务器上，用来负责处理集群的元数据和协调工作。
- Historicals和MiddleManagers可以部署在同一台服务器上，用来负责处理集群中的用户数据。
- Brokers负责接收查询并转发到集群其他节点，它也提供了查询的缓存策略。

环境：
    JAVA7或更高版本

安装：
    
在其中一个服务器上下载并配置config，然后再把修改配置后的copy到其他服务器上。

```
curl -O http://static.druid.io/artifacts/releases/druid-0.9.1.1-bin.tar.gz
tar -xzf druid-0.9.1.1-bin.tar.gz
cd druid-0.9.1.1
```

和单击环境一样，在这个目录下会发现以下文件

```
LICENSE - license 文件.
bin/ - 启动脚本
conf/* -集群配置
conf-quickstart/* - quickstart.的配置
extensions/* - 所有Druid的扩展库.
hadoop-dependencies/* - Druid 的 Hadoop 依赖.
lib/* - Druid的所有核心包.
quickstart/* - quickstart启动的相关文件
```

接下来需要更改conf的相关配置
 ### 1. 配置Deep存储

    Deep存储官方主要给了2种，S3和HDFS。S3是AWS常用的，一般还是HDFS比较多。我们这边集群也是采用HDFS。下面说下这两种的配置方式。
#### S3
需要修改conf/druid/_common/common.runtime.properties配置文件：

- 设置 druid.extensions.loadList=["druid-s3-extensions"]
- 注释掉本地local的Deep storage和Indexing service logs的配置
- 取消S3下的Deep storage和Indexing service logs的配置
 
设置完成之后如下：

```
druid.extensions.loadList=["druid-s3-extensions"]
#druid.storage.type=local
#druid.storage.storageDirectory=var/druid/segments
druid.storage.type=s3
druid.storage.bucket=your-bucket
druid.storage.baseKey=druid/segments
druid.s3.accessKey=...
druid.s3.secretKey=...
#druid.indexer.logs.type=file
#druid.indexer.logs.directory=var/druid/indexing-logs
druid.indexer.logs.type=s3
druid.indexer.logs.s3Bucket=your-bucket
druid.indexer.logs.s3Prefix=druid/indexing-logs
```

####   HDFS
HDFS和上面的修改方式基本一样。设置后如下所示：

```
druid.extensions.loadList=["druid-hdfs-storage"]
#druid.storage.type=local
#druid.storage.storageDirectory=var/druid/segments
druid.storage.type=hdfs
druid.storage.storageDirectory=/druid/segments
#druid.indexer.logs.type=file
#druid.indexer.logs.directory=var/druid/indexing-logs
druid.indexer.logs.type=hdfs
druid.indexer.logs.directory=/druid/indexing-logs
```

同时，HDFS的一些配置文件需要copy到 conf/druid/_common/ 目录下
这四个主要配置文件：core-site.xml, hdfs-site.xml, yarn-site.xml, mapred-site.xml

### 2.    配置Tranquility Server（可选）
这个是流数据处理服务，可以通过Tranquility的API，将自己的Java程序处理的数据写入。在kafka等数据不能直接完全写入的时候可以用到，我们的数据就得从kafka获取后再进行更改才能写入。

### 3.    配置Tranquility Kafka（可选）
如果数据可以直接从kafka不加工就进入Druid，那么可以直接配置kafka源。

### 4.    配置连接Hadoop（可选）
如果数据是从Hadoop集群里面加载到druid，那么就需要配置druid来感知Hadoop集群。

更新 conf/middleManager/runtime.properties 下的 druid.indexer.task.hadoopWorkingPath 配置项，例如：

```
ruid.indexer.task.hadoopWorkingPath=/tmp/druid-indexing
```
copy Hadoop的配置文件到druid的节点下，依旧是上面提到的四个文件。例如：conf/druid/_common/core-site.xml, conf/druid/_common/hdfs-site.xml

### 5.    配置coordination的地址
在一个简单的集群中，可以在同一台服务器配置一个Coordinator节点、一个Overload节点、一个Zookeeper实例，以及元数据存储。

在conf/druid/_common/common.runtime.properties配置文件中，修改zk的host为Zookeeper实例运行的主机IP
druid.zk.service.host
    同样，在conf/_common/common.runtime.properties配置文件中，修改元数据的存储IP

```
druid.metadata.storage.connector.connectURI
druid.metadata.storage.connector.host
```

注：在生产环境中，为了稳定高可用，还是配置2个主服务，每台服务器都跑一个Coordinator和Overload节点，Zookeeper也可以多开2个实例。

### 6.    调节Druid的服务运行进程
Druid的Historicals和MiddleManagers节点可以部署在同一台服务器，通过调节参数可以让节点更好的运行。如果是使用的Tranquility Server或者Kafka，也可以部署在一起。
    下面是官方提到的常用的参数调节：

```
-Xmx and -Xms
druid.server.http.numThreads
druid.processing.buffer.sizeBytes
druid.processing.numThreads
druid.query.groupBy.maxIntermediateRows
druid.query.groupBy.maxResults
druid.server.maxSize and druid.segmentCache.locations on Historical Nodes
druid.worker.capacity on MiddleManagers
```

注意：需要保证参数 -XX:MaxDirectMemory >= numThreads*sizeBytes, 否则druid启动不起来。

### 7.    开放端口（如果开启了防火墙）
需要开放一下端口：

```
1527 使用MySQL或者PostgreSQL就不需要)
2181 (ZooKeeper; 如果使用自己的ZooKeeper集群就不需要)
8081 (Coordinator)
8082 (Broker)
8083 (Historical)
8084 (Standalone Realtime, if used)
8088 (Router, if used)
8090 (Overlord)
8091, 8100–8199 (Druid Middle Manager; you may need higher than port 8199 if you have a very high druid.worker.capacity)
8200 (Tranquility Server, if used)
```

生产环境下，元数据存储和Zookeeper都采用自己配置的就可以了。

### 8.    启动节点
####    8.1 启动Coordinator, Overlord, Zookeeper, 和元数据存储节点。
拷贝druid的文件和修改配置到coordination服务器
    拷贝可以采用rsync或者scp方式

```
rsync -az druid-0.9.1.1/ COORDINATION_SERVER:druid-0.9.1.1/
```

 登陆到coordination服务器上并安装Zookeeper

```
curl http://www.gtlib.gatech.edu/pub/apache/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz -o zookeeper-3.4.6.tar.gz
tar -xzf zookeeper-3.4.6.tar.gz
cd zookeeper-3.4.6
cp conf/zoo_sample.cfg conf/zoo.cfg
./bin/zkServer.sh start
```

生产环境为了HA，可以设置一个Zookeeper集群来维护。

在coordination服务节点上，进入目录启动coordination服务，由于此方式是非守护进行，所以得开两个窗口进行分别启动。或者直接将启动的信息管道到其指定的文件夹中。

```
java `cat conf/druid/coordinator/jvm.config | xargs` -cp conf/druid/_common:conf/druid/coordinator:lib/* io.druid.cli.Main server coordinator
java `cat conf/druid/overlord/jvm.config | xargs` -cp conf/druid/_common:conf/druid/overlord:lib/* io.druid.cli.Main server overlord
```

#### 8.2 启动Historicals和MiddleManagers节点
同样，复制修改和配置好的druid文件到需要部署的historical和MiddleManagers服务器上，然后启动这2个节点：

```
java `cat conf/druid/historical/jvm.config | xargs` -cp conf/druid/_common:conf/druid/historical:lib/* io.druid.cli.Main server historical
java `cat conf/druid/middleManager/jvm.config | xargs` -cp conf/druid/_common:conf/druid/middleManager:lib/* io.druid.cli.Main server middleManager
```

如果是用kafka或者HTTP的流方式摄入数据，那就需要启动Tranquility服务，和上面2个节点放在一个服务器上就可以了。就算对于大规模的生产环境，MiddleManager和Tranquility也可以部署到同一台服务器。当然如果使用一个进程引入了Tranquility进行流数据摄入，那就可以不需要启动Tranquility服务。

```
curl -O http://static.druid.io/tranquility/releases/tranquility-distribution-0.8.0.tgz
tar -xzf tranquility-distribution-0.8.0.tgz
cd tranquility-distribution-0.8.0
bin/tranquility <server or kafka> -configFile <path_to_druid_distro>/conf/tranquility/<server or kafka>.json
```

#### 8.3 启动broker节点

同样把修改后的druid文件复制到需要启动broker节点的服务器上，然后启动broker服务

```
java `cat conf/druid/broker/jvm.config | xargs` -cp conf/druid/_common:conf/druid/broker:lib/* io.druid.cli.Main server broker
```

You can add more Brokers as needed based on query load.
